{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "broken-indonesia",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook we will train the 3D Unet to segment the liver and liver tumors in CT scans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-essay",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "* pathlib for easy path handling\n",
    "* HTML for visualizing volume videos\n",
    "* torchio for dataset creation\n",
    "* torch for DataLoaders, optimizer and loss\n",
    "* pytorch-lightning for training\n",
    "* numpy for masking\n",
    "* matplotlib for visualization\n",
    "* Our 3D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tropical-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torchio as tio\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from model import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3965e6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchio\n",
      "  Downloading torchio-0.19.2-py2.py3-none-any.whl.metadata (47 kB)\n",
      "     ---------------------------------------- 0.0/47.2 kB ? eta -:--:--\n",
      "     ---------------------------------- ----- 41.0/47.2 kB ? eta -:--:--\n",
      "     ---------------------------------- ----- 41.0/47.2 kB ? eta -:--:--\n",
      "     ---------------------------------- ----- 41.0/47.2 kB ? eta -:--:--\n",
      "     ---------------------------------- ----- 41.0/47.2 kB ? eta -:--:--\n",
      "     ---------------------------------- ----- 41.0/47.2 kB ? eta -:--:--\n",
      "     ---------------------------------- ----- 41.0/47.2 kB ? eta -:--:--\n",
      "     ---------------------------------- ----- 41.0/47.2 kB ? eta -:--:--\n",
      "     ---------------------------------- ----- 41.0/47.2 kB ? eta -:--:--\n",
      "     ---------------------------------- ----- 41.0/47.2 kB ? eta -:--:--\n",
      "     --------------------------------------- 47.2/47.2 kB 90.9 kB/s eta 0:00:00\n",
      "Collecting Deprecated (from torchio)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting SimpleITK!=2.0.*,!=2.1.1.1 (from torchio)\n",
      "  Downloading SimpleITK-2.3.1-cp39-cp39-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting humanize (from torchio)\n",
      "  Downloading humanize-4.8.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: nibabel in c:\\users\\jaikr\\.conda\\envs\\jupyter_1\\lib\\site-packages (from torchio) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\jaikr\\.conda\\envs\\jupyter_1\\lib\\site-packages (from torchio) (1.26.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\jaikr\\.conda\\envs\\jupyter_1\\lib\\site-packages (from torchio) (1.11.3)\n",
      "Requirement already satisfied: torch>=1.1 in c:\\users\\jaikr\\.conda\\envs\\jupyter_1\\lib\\site-packages (from torchio) (2.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jaikr\\.conda\\envs\\jupyter_1\\lib\\site-packages (from torchio) (4.65.0)\n",
      "Collecting typer[all] (from torchio)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/45.9 kB ? eta -:--:--\n",
      "     --------------------------------- ---- 41.0/45.9 kB 991.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 45.9/45.9 kB 565.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\jaikr\\.conda\\envs\\jupyter_1\\lib\\site-packages (from torch>=1.1->torchio) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jaikr\\.conda\\envs\\jupyter_1\\lib\\site-packages (from torch>=1.1->torchio) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\jaikr\\.conda\\envs\\jupyter_1\\lib\\site-packages (from torch>=1.1->torchio) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\jaikr\\.conda\\envs\\jupyter_1\\lib\\site-packages (from torch>=1.1->torchio) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jaikr\\.conda\\envs\\jupyter_1\\lib\\site-packages (from torch>=1.1->torchio) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jaikr\\.conda\\envs\\jupyter_1\\lib\\site-packages (from torch>=1.1->torchio) (2023.10.0)\n",
      "Collecting wrapt<2,>=1.10 (from Deprecated->torchio)\n",
      "  Downloading wrapt-1.16.0-cp39-cp39-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: packaging>=17 in c:\\users\\jaikr\\.conda\\envs\\jupyter_1\\lib\\site-packages (from nibabel->torchio) (23.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jaikr\\.conda\\envs\\jupyter_1\\lib\\site-packages (from tqdm->torchio) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\jaikr\\.conda\\envs\\jupyter_1\\lib\\site-packages (from typer[all]->torchio) (8.1.7)\n",
      "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]->torchio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in c:\\users\\jaikr\\.conda\\envs\\jupyter_1\\lib\\site-packages (from typer[all]->torchio) (13.6.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jaikr\\.conda\\envs\\jupyter_1\\lib\\site-packages (from rich<14.0.0,>=10.11.0->typer[all]->torchio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jaikr\\appdata\\roaming\\python\\python39\\site-packages (from rich<14.0.0,>=10.11.0->typer[all]->torchio) (2.16.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jaikr\\.conda\\envs\\jupyter_1\\lib\\site-packages (from jinja2->torch>=1.1->torchio) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jaikr\\.conda\\envs\\jupyter_1\\lib\\site-packages (from sympy->torch>=1.1->torchio) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jaikr\\.conda\\envs\\jupyter_1\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]->torchio) (0.1.2)\n",
      "Downloading torchio-0.19.2-py2.py3-none-any.whl (172 kB)\n",
      "   ---------------------------------------- 0.0/172.8 kB ? eta -:--:--\n",
      "   -------------------------- ------------- 112.6/172.8 kB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 172.8/172.8 kB 3.5 MB/s eta 0:00:00\n",
      "Downloading SimpleITK-2.3.1-cp39-cp39-win_amd64.whl (18.1 MB)\n",
      "   ---------------------------------------- 0.0/18.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/18.1 MB 5.5 MB/s eta 0:00:04\n",
      "   ---------------------------------------- 0.2/18.1 MB 2.8 MB/s eta 0:00:07\n",
      "    --------------------------------------- 0.4/18.1 MB 3.4 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.5/18.1 MB 3.4 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.7/18.1 MB 3.5 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.7/18.1 MB 3.5 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.7/18.1 MB 3.5 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.8/18.1 MB 2.3 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.9/18.1 MB 2.3 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 1.0/18.1 MB 2.3 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 1.2/18.1 MB 2.6 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.5/18.1 MB 2.8 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.7/18.1 MB 3.0 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.0/18.1 MB 3.3 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 2.4/18.1 MB 3.5 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 2.7/18.1 MB 3.8 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 3.2/18.1 MB 4.1 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 3.5/18.1 MB 4.3 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 3.9/18.1 MB 4.6 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 4.1/18.1 MB 4.6 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 4.1/18.1 MB 4.6 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 4.5/18.1 MB 4.4 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 4.6/18.1 MB 4.4 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 4.6/18.1 MB 4.4 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 4.9/18.1 MB 4.3 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 5.0/18.1 MB 4.3 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 5.0/18.1 MB 4.1 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 5.4/18.1 MB 4.2 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 5.7/18.1 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 6.0/18.1 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 6.0/18.1 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 6.0/18.1 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 6.0/18.1 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 6.3/18.1 MB 4.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 6.5/18.1 MB 4.1 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 6.6/18.1 MB 4.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 6.7/18.1 MB 4.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 6.9/18.1 MB 3.9 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 6.9/18.1 MB 4.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 7.0/18.1 MB 3.8 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 7.0/18.1 MB 3.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 7.3/18.1 MB 3.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 7.3/18.1 MB 3.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 7.3/18.1 MB 3.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 7.3/18.1 MB 3.5 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 7.6/18.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 8.0/18.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 8.3/18.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 8.3/18.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 8.3/18.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 8.3/18.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 8.3/18.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 8.4/18.1 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 8.9/18.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 9.0/18.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 9.0/18.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 9.0/18.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 9.0/18.1 MB 3.4 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 9.2/18.1 MB 3.4 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 9.2/18.1 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 9.7/18.1 MB 3.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 9.7/18.1 MB 3.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 9.7/18.1 MB 3.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 9.8/18.1 MB 3.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 9.9/18.1 MB 3.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 10.2/18.1 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 10.4/18.1 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 10.4/18.1 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 10.4/18.1 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 10.4/18.1 MB 3.4 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 10.4/18.1 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 10.8/18.1 MB 3.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 11.0/18.1 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 11.0/18.1 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 11.1/18.1 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 11.3/18.1 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 11.4/18.1 MB 3.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 11.8/18.1 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 12.2/18.1 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 12.2/18.1 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 12.2/18.1 MB 3.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 12.4/18.1 MB 3.2 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 12.4/18.1 MB 3.2 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 12.4/18.1 MB 3.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 13.1/18.1 MB 3.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 13.2/18.1 MB 3.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 13.2/18.1 MB 3.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 13.3/18.1 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 13.6/18.1 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 13.9/18.1 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 14.3/18.1 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 14.4/18.1 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 14.4/18.1 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 14.4/18.1 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 14.4/18.1 MB 3.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 14.5/18.1 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 14.7/18.1 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 15.0/18.1 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 15.0/18.1 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 15.1/18.1 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 15.2/18.1 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 15.2/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 15.6/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 15.9/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 15.9/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 16.0/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 16.1/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 16.1/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 16.3/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 16.5/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 16.6/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 16.6/18.1 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 16.7/18.1 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 16.7/18.1 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 17.1/18.1 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.4/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.4/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.4/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.4/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.5/18.1 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  17.8/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.1/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.1/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.1/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.1/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.1/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.1/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.1/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.1/18.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.1/18.1 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading humanize-4.8.0-py3-none-any.whl (117 kB)\n",
      "   ---------------------------------------- 0.0/117.1 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 41.0/117.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 117.1/117.1 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading wrapt-1.16.0-cp39-cp39-win_amd64.whl (37 kB)\n",
      "Installing collected packages: SimpleITK, wrapt, shellingham, humanize, typer, Deprecated, torchio\n",
      "Successfully installed Deprecated-1.2.14 SimpleITK-2.3.1 humanize-4.8.0 shellingham-1.5.4 torchio-0.19.2 typer-0.9.0 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-canon",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "We can loop over all available scans and add them to the subject list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "square-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_img_to_label_path(path):\n",
    "    \"\"\"\n",
    "    Replace data with mask to get the masks\n",
    "    \"\"\"\n",
    "    parts = list(path.parts)\n",
    "    parts[parts.index(\"imagesTr\")] = \"labelsTr\"\n",
    "    return Path(*parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rapid-plastic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = Path(\"Task03_Liver_rs/imagesTr/\")\n",
    "subjects_paths = list(path.glob(\"liver_*\"))\n",
    "subjects = []\n",
    "\n",
    "for subject_path in subjects_paths:\n",
    "    label_path = change_img_to_label_path(subject_path)\n",
    "    subject = tio.Subject({\"CT\":tio.ScalarImage(subject_path), \"Label\":tio.LabelMap(label_path)})\n",
    "    subjects.append(subject)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc56abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for subject in subjects:\n",
    "#     assert subject[\"CT\"].orientation == (\"R\", \"A\", \"S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-bread",
   "metadata": {},
   "source": [
    "We use the same  augmentation steps as used in the Dataset notebook. <br />\n",
    "Regarding the processing, we use the **CropOrPad** functionality which crops or pads all images and masks to the same shape. <br />\n",
    "\n",
    "We use ($256 \\times 256 \\times 200$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hearing-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = tio.Compose([\n",
    "            tio.CropOrPad((256, 256, 200)),\n",
    "            tio.RescaleIntensity((-1, 1))\n",
    "            ])\n",
    "\n",
    "\n",
    "augmentation = tio.RandomAffine(scales=(0.9, 1.1), degrees=(-10, 10))\n",
    "\n",
    "\n",
    "val_transform = process\n",
    "train_transform = tio.Compose([process, augmentation])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-construction",
   "metadata": {},
   "source": [
    "Define the train and validation dataset. We use 105 subjects for training and 13 for testing. <br />\n",
    "In order to help the segmentation network learn, we use the LabelSampler with p=0.2 for background, p=0.3 for liver and p=0.5 for liver tumors with a patch size of ($96 \\times 96 \\times 96$).\n",
    "\n",
    "Feel free to try the UniformSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ethical-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tio.SubjectsDataset(subjects[:105], transform=train_transform)\n",
    "val_dataset = tio.SubjectsDataset(subjects[105:], transform=val_transform)\n",
    "\n",
    "sampler = tio.data.LabelSampler(patch_size=96, label_name=\"Label\", label_probabilities={0:0.2, 1:0.3, 2:0.5})\n",
    "#sampler = tio.data.UniformSampler(patch_size=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-smile",
   "metadata": {},
   "source": [
    "Create the queue to draw patches from.<br />\n",
    "The tio.Queue accepts a SubjectsDataset, a max_length argument describing the the number of patches that can be stored, the number of patches to draw from each subject, a sampler and the number of workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "after-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Adapt max_length and num_workers to your hardware\n",
    "\n",
    "train_patches_queue = tio.Queue(\n",
    "     train_dataset,\n",
    "     max_length=40,\n",
    "     samples_per_volume=5,\n",
    "     sampler=sampler,\n",
    "     num_workers=4,\n",
    "    )\n",
    "\n",
    "val_patches_queue = tio.Queue(\n",
    "     val_dataset,\n",
    "     max_length=40,\n",
    "     samples_per_volume=5,\n",
    "     sampler=sampler,\n",
    "     num_workers=4,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-finding",
   "metadata": {},
   "source": [
    "Define train and val loader:\n",
    "\n",
    "NOTE: As the dataloaders only pop patches from the queue use 0 num workers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beneficial-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, adapt batch size according to your hardware\n",
    "batch_size = 2\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_patches_queue, batch_size=batch_size, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_patches_queue, batch_size=batch_size, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-midwest",
   "metadata": {},
   "source": [
    "Finally we can create the Segmentation model.\n",
    "\n",
    "We use the Adam optimizer with a learning rate of 1e-4 and a weighted cross-entropy loss, which assigns a threefold increased loss to tumorous voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "third-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmenter(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = UNet()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, data):\n",
    "        pred = self.model(data)\n",
    "        return pred\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # You can obtain the raw volume arrays by accessing the data attribute of the subject\n",
    "        img = batch[\"CT\"][\"data\"]\n",
    "        mask = batch[\"Label\"][\"data\"][:,0]  # Remove single channel as CrossEntropyLoss expects NxHxW\n",
    "        mask = mask.long()\n",
    "        \n",
    "        pred = self(img)\n",
    "        loss = self.loss_fn(pred, mask)\n",
    "        \n",
    "        # Logs\n",
    "        self.log(\"Train Loss\", loss)\n",
    "        if batch_idx % 50 == 0:\n",
    "            self.log_images(img.cpu(), pred.cpu(), mask.cpu(), \"Train\")\n",
    "        return loss\n",
    "    \n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # You can obtain the raw volume arrays by accessing the data attribute of the subject\n",
    "        img = batch[\"CT\"][\"data\"]\n",
    "        mask = batch[\"Label\"][\"data\"][:,0]  # Remove single channel as CrossEntropyLoss expects NxHxW\n",
    "        mask = mask.long()\n",
    "        \n",
    "        pred = self(img)\n",
    "        loss = self.loss_fn(pred, mask)\n",
    "        \n",
    "        # Logs\n",
    "        self.log(\"Val Loss\", loss)\n",
    "        self.log_images(img.cpu(), pred.cpu(), mask.cpu(), \"Val\")\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def log_images(self, img, pred, mask, name):\n",
    "        \n",
    "        results = []\n",
    "        pred = torch.argmax(pred, 1) # Take the output with the highest value\n",
    "        axial_slice = 50  # Always plot slice 50 of the 96 slices\n",
    "        \n",
    "        fig, axis = plt.subplots(1, 2)\n",
    "        axis[0].imshow(img[0][0][:,:,axial_slice], cmap=\"bone\")\n",
    "        mask_ = np.ma.masked_where(mask[0][:,:,axial_slice]==0, mask[0][:,:,axial_slice])\n",
    "        axis[0].imshow(mask_, alpha=0.6)\n",
    "        axis[0].set_title(\"Ground Truth\")\n",
    "        \n",
    "        axis[1].imshow(img[0][0][:,:,axial_slice], cmap=\"bone\")\n",
    "        mask_ = np.ma.masked_where(pred[0][:,:,axial_slice]==0, pred[0][:,:,axial_slice])\n",
    "        axis[1].imshow(mask_, alpha=0.6, cmap=\"autumn\")\n",
    "        axis[1].set_title(\"Pred\")\n",
    "\n",
    "        self.logger.experiment.add_figure(f\"{name} Prediction vs Label\", fig, self.global_step)\n",
    "\n",
    "            \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        #Caution! You always need to return a list here (just pack your optimizer into one :))\n",
    "        return [self.optimizer]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "first-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the model\n",
    "model = Segmenter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "lovely-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='Val Loss',\n",
    "    save_top_k=10,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cardiac-mouth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Create the trainer\n",
    "# Change the gpus parameter to the number of available gpus in your computer. Use 0 for CPU training\n",
    "\n",
    "gpus = 1 #TODO\n",
    "trainer = pl.Trainer( logger=TensorBoardLogger(save_dir=\"./logs\"), log_every_n_steps=1,\n",
    "                     callbacks=checkpoint_callback,\n",
    "                     max_epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "finite-superintendent",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: ./logs\\lightning_logs\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | model   | UNet             | 5.8 M \n",
      "1 | loss_fn | CrossEntropyLoss | 0     \n",
      "---------------------------------------------\n",
      "5.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.8 M     Total params\n",
      "23.344    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28966026b838481f85d4d48f627c701d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                               | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaikr\\.conda\\envs\\Jupyter_1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\jaikr\\.conda\\envs\\Jupyter_1\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "# This might take some hours depending on your GPU\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-favorite",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "loving-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from celluloid import Camera\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-memphis",
   "metadata": {},
   "source": [
    "First we load the model and place it on the gpu if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "competent-passage",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaikr\\.conda\\envs\\Jupyter_1\\lib\\site-packages\\pytorch_lightning\\utilities\\migration\\migration.py:207: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.1.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint D:\\Udemy-Medical PyTorch\\08-3D-Liver-Tumor-Segmentation\\08-3D-Liver-Tumor-Segmentation\\weights\\epoch=97-step=25773.ckpt`\n"
     ]
    }
   ],
   "source": [
    "model = Segmenter.load_from_checkpoint(\"weights/epoch=97-step=25773.ckpt\")\n",
    "model = model.eval()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-korea",
   "metadata": {},
   "source": [
    "### Patch Aggregation\n",
    "The model was trained in a patch wise manner as the full volumes are too large to be placed on a typical GPU.\n",
    "But we still want to get a result for the whole volume.<br />\n",
    "torchio helps us doing so by performing *Patch Aggregation*\n",
    "\n",
    "The goal of patch aggregation is to split the image into patches, then compute the segmentation for each patch and finally merge the predictions into the prediction for the full volume.\n",
    "\n",
    "The pipeline is as follows:\n",
    "1. Define the **GridSampler(subject, patch_size, patch_overlap)** responsible for dividing the volume into patches. Each patch is defined by its location accesible via *tio.LOCATION*\n",
    "2. Define the **GridAggregator(grid_sampler)** which merges the predicted patches back together\n",
    "3. Compute the prediction on the patches and aggregate them via **aggregator.add_batch(pred, location)**\n",
    "4. Extract the full prediction via **aggregator.get_output_tensor()**\n",
    "\n",
    "Additionally, we can leverage the DataLoader from pytorch to perform the prediction in a batch wise manner for a nice speed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "integrated-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a validation subject and extract the images and segmentation for evaluation\n",
    "IDX = 4\n",
    "mask = val_dataset[IDX][\"Label\"][\"data\"]\n",
    "imgs = val_dataset[IDX][\"CT\"][\"data\"]\n",
    "\n",
    "# GridSampler\n",
    "grid_sampler = tio.inference.GridSampler(val_dataset[IDX], 96, (8, 8, 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "impressive-times",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridAggregator\n",
    "aggregator = tio.inference.GridAggregator(grid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "regulation-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader for speed up\n",
    "patch_loader = torch.utils.data.DataLoader(grid_sampler, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "logical-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "with torch.no_grad():\n",
    "    for patches_batch in patch_loader:\n",
    "        input_tensor = patches_batch['CT'][\"data\"].to(device)  # Get batch of patches\n",
    "        locations = patches_batch[tio.LOCATION]  # Get locations of patches\n",
    "        pred = model(input_tensor)  # Compute prediction\n",
    "        aggregator.add_batch(pred, locations)  # Combine predictions to volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "neural-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the volume prediction\n",
    "output_tensor = aggregator.get_output_tensor()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-planner",
   "metadata": {},
   "source": [
    "Finally we can visualize the prediction as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "electronic-horse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "camera = Camera(fig)  # create the camera object from celluloid\n",
    "pred = output_tensor.argmax(0)\n",
    "\n",
    "for i in range(0, output_tensor.shape[3], 2):  # axial view\n",
    "    plt.imshow(imgs[0,:,:,i], cmap=\"bone\")\n",
    "    mask_ = np.ma.masked_where(pred[:,:,i]==0, pred[:,:,i])\n",
    "    label_mask = np.ma.masked_where(mask[0,:,:,i]==0, mask[0,:,:,i])\n",
    "    plt.imshow(mask_, alpha=0.1, cmap=\"autumn\")\n",
    "    #plt.imshow(label_mask, alpha=0.5, cmap=\"jet\")  # Uncomment if you want to see the label\n",
    "\n",
    "    # plt.axis(\"off\")\n",
    "    camera.snap()  # Store the current slice\n",
    "animation = camera.animate()  # create the animation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(animation.to_html5_video())  # convert the animation to a video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-volleyball",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
